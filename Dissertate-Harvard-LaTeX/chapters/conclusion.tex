%!TEX root = ../dissertation.tex
\chapter{Conclusion}
\label{conclusion}
Through the use of tight theoretical bounds and clever data structure optimizations, we are able to find and certify the optimal rule list on real-world problems.
We have also shown that other rule list methods produce rule lists which are close to optimal.
On two problems plagued by racially biased models in the real world, we show that interpretable methods achieve comparable accuracy to black-box models, refuting the claim that black-box models lead to better performance.
The rule lists generated by our algorithm had no race-related factors, supporting their use over racially biased, uninterpretable models.

We also showed that careful analysis of where memory is allocated is especially important for combinatorial optimization problems.
Even with a good branch and bound algorithm and a modern machine, large datasets will test the memory constraints of that machine.
We have provided an novel data structure, the symmetry-aware map, for future combinatorial optimization problems.
Potential future optimizations for the map include distance sensitive hashing or bit-packing to reduce runtime and memory usage further.

This work has also provided a parallel implementation of CORELS.
The techniques used to parallelize this algorithm are not rule list specific and could be applied to any branch and bound algorithm.
Even with a sub-linear speedup, the parallel implementation of CORELS provides large runtime savings and could be useful when applying CORELS to real world problems.
Further work remains to be done on the analysis of the slowdown as well as potentially parallelizing the implementation across machines.

%This idea of deterministically finding the optimal rule list has been around for a while, but it has not been feasible until recently.
%Our novel bounds and use of branch-and-bound technique applied to the problem of discovering the optimal rule list show the renewed usefulness of applying discrete optimization approaches to problems.

Discrete optimization has become less popular as other techniques such as convex optimization or stochastic gradient descent dominate the machine learning landscape.
However, we showed that due to dramatic increases in processor speed and computer memory, discrete optimization techniques can be applied to real problems and complete them in a reasonable amount of time.
In particular, we hope that this work will inspire further work on discrete optimization techniques for other methods such as decision trees or SVMs.