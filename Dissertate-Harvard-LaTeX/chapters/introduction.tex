%!TEX root = ../dissertation.tex
\chapter{Introduction}
\label{introduction}

As machine learning continues to grow in importance, the interpretability of predictive models remains a crucial problem.
Our goal is to build models that are highly predictive but in which each step of the model's decision making process can also be understood by humans.
Machine learning models such as neural nets or support vector machines are able to achieve stunning predictive accuracy, but the reasons for these predictions remain unintelligible to a human user.
This lack of interpretability is important because models that are not understood by humans can have hidden bias in their predictive decision making.
A recent ProPublica article found racial bias in the use of black box machine learning models used for advising criminal sentencing \cite{LarsonMaKiAn16}.
Northepointe, the company which provides a black box model called COMPAS argues that their use of a black box model is necessitated by the fact that they can achieve better accuracy through the use of that model.
This thesis is part of a body of work that hopes to prove that interpretability can be achieved without sacrificing accuracy.
In particular, this thesis will examine the data structure optimizations used to run this algorithm efficiently.

To achieve interpretability, we use \emph{rule lists}, also known as decision lists, which are lists comprised of \emph{if-then} statements \cite{Rivest87}. 
This structure allows for predictive models that also can be easily interpreted because the rules that are satisfied give a reason for each prediction. 
Given a set of rules associated with a dataset, rule lists can be constructed by placing the rules in different orders.
Since most data points are captured by multiple rules, changing the order of rules leads to different predictions and therefore different accuracies. 
Different rule list algorithms all attempt to maximize predictive accuracy through the discovery of different rules lists.

Pitting interpretable models against black box ones raises the question of how accurate the interpretable models can be.
Thus, searching for an optimal model to provide an upper bound on the potential accuracy of an interpretable model is crucial when discussing whether or not to use interpretable models.
In our case, we are searching for the  rule list with the highest accuracy, the optimal rule list. 
A brute force solution to find the the optimal rule list would be computationally prohibitive due to the combinatorially number of rule lists.
Our goal is to create an algorithm to find the optimal rule list in a reasonable amount of time.

Recent work on generating rule lists \cite{LethamRuMcMa15,YangRuSe16} uses
probabilistic approaches to generating rule lists.
These approaches achieve high accuracy while also managing to run quickly. 
However, despite the apparent accuracy of the rule lists generated by these algorithms, there is no way to determine if the generated rule list is optimal or how close to optimal the rule lists is. 
Our model, called Certifiably Optimal RulE ListS (CORELS), finds the optimal rule list and allows us to also investigate the accuracy of near optimal solutions. 
The benefits of this model are two-fold: firstly, we are able to generate the best rule list on a given data set and therefore will have the most accurate predictions that a rule list can give.
Secondly, since CORELS generates the entire space of potential solutions, we can judge how good rule lists generated by other algorithms are. 
In particular, we can investigate if the rule lists from probabilistic approaches are nearly optimal or whether those approaches sacrifice too much accuracy for speed.
This will allows us to bound the accuracy on important problems and determine if interpretable methods shuold be used.

CORELS achieves these results by placing a bound on the best performance that a rule list can achieve in the future. 
This allows us to prune that rule list if that bound is worse than the objective value of the best rule list that we have already examined.
We continue to look at rule lists until we have either examined every rule list or eliminated all but one from consideration. 
Thus, when the algorithm terminates, we have found the rule list with the best possible accuracy. 
Our use of this branch and bound technique leads to massive pruning of the search space of potential rule lists and means our algorithm can find the optimal rule list on real data sets.

Due to our interest in interpretability, the amount of data each rule captures informs the value of that rule. 
We want our rule lists to be understandable by humans, so shorter rule lists are more optimal. 
Therefore, we use an objective function that takes into account both accuracy and the length of the rule list to prevent overfitting. 
This means we may not always find the highest accuracy rule list--our optimality is over both accuracy and length of rule lists.
This requires each rule to capture a certain amount of data correctly to make it worth the penalty of making our rule list longer. 
This limits the overall length of our rule lists and prevents us from investigating rule lists containing useless rules.

The exponential nature of the problem means that the efficacy of CORELS is dependent on how much our bounds allow us to prune. 
We list a few types of bounds that allow us to drastically prune our search space. 
The first type of bound is intrinsic to the rules themselves. 
This category includes bounds such the bound described above that ensures rules capture enough data correctly to overcome a regularization parameter. 
Our second type of bound compares the best future performance of a given rule list to the best solution encountered so far. 
We can avoid examining parts of the search space whose maximum possible accuracy is less than the accuracy of our current best solution. 
Finally, our last class of bounds uses a symmetry-aware map to prune all but the best permutation of any given set of rules.

To keep track of all of these bounds for each rule list, we implemented a modified trie that we call a prefix tree. 
Each node in the prefix tree represents an individual rule; thus, each path in the tree represents a rule list where the final node in the path contains the metrics about that rule list.
This tree structure facilities the use of multiple different selection algorithms including breadth-first search, a priority queue based on a custom function that trades off exploration and exploitation, and a stochastic selection process. 
In addition, we are able to limit the number of nodes in the tree and thereby achieve a way of tuning space-time tradeoffs in a robust manner. 
We propose that this tree structure is a useful way of organizing the generation of rule lists and allows the implementation of CORELS to be easily parallelized.

We evaluated CORELS on a number of datasets from the UCI repository. 
Our metric of success was prediction accuracy on a subset of the data which we calculated using 10-fold cross validation. 
These datasets involve hundreds of rules and hundreds or thousands of samples and CORELS is able to find the optimal rule list within 10 minutes. 
We show that we are able to achieve better accuracy on these datasets than the popular greedy algorithms, CART or C4.5.

In addition, we applied CORELS to the problem of predicting criminal recidivism. 
Larson et al examines the problem of predicting recidivism and shows that a black box model, specifically the COMPAS score from the company Northpointe, has racially biased prediction \cite{LarsonMaKiAn16}.
Black defendants are misclassified at a higher risk for recidivism than in actuality, while white defendants are misclassified at a lower risk. 
The model which produces the COMPAS scores is a black box algorithm which is not interpretable, and therefore the model does not provide a way for human input to correct for these racial biases. 
Our model produces similar accuracies to the predictive models and COMPAS scores from Larson et al while maintaining its interpretability.

CORELS demonstrates a novel approach towards generating interpretable models by looking for the optimal rule list. 
While searching for that optimal list, we are able to discover near-optimal solutions that provide insight into how effective other interpretable methods might be. 
We have proved numerous bounds that allow us to prune rule lists.
Finally, we created a novel symmetry-aware map that drastically reduces the search space of possible rule lists.

Chapter 2 provides an overview of related work in the fields of rule lists, interpretable models, and discrete optimization. 
Chapter 3 proves definitions that underlie the execution of the algorithm.
Chapter 4 describes the implementation of the algorithm, emphasizing the data structures used to make this problem tractable.
Chapter 5 describes experiments run to test the space/time tradeoffs detailed in chapter 4.