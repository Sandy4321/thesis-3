%!TEX root = ../dissertation.tex
%\begin{savequote}[75mm]
%Nulla facilisi. In vel sem. Morbi id urna in diam dignissim feugiat. Proin molestie tortor eu velit. Aliquam erat volutpat. Nullam ultrices, diam tempus vulputate egestas, eros pede varius leo.
%\qauthor{Quoteauthor Lastname}
%\end{savequote}

\chapter{Related Work}

The use of classification models is popular in a number of different fields from image recognition to churn prediction.
Oftentimes, however, simply receiving a prediction from software is not enough--it is important to have a predictive model that humans can investigate and understand \cite{Ruping06, Bratko97, Quinlan99, Martens11, Freitas14}.
For example, in fields such as medical diagnoses \cite{BellazziZu08} and criminal sentencing \cite{LarsonMaKiAn16}, it is important to be able to investigate the reasons behind a model's predictions.
One reason is that medical experts are unlikely to trust the predictions of these models if they are unable to understand why the model is making certain predictions \cite{Lavrač99}.
Interpretable models also allow users to examine predictions to detect systemic biases in the model.
This is especially important in classification problems such as criminal recidivism prediction where there are often race-related biases\cite{LarsonMaKiAn16} or credit scoring where a justification is necessary for the denial of credit\cite{BaesensMuDeVaSe05}.

Tree structured classifiers are a popular technique that combines interpretability with a high predictive accuracy.
Also called decision trees, these trees are often used as either classification or regression tools.
Every node in the tree classifier splits the data into two subsets; these subsets are then recursively split by nodes lower in the tree.
Nodes are constructed by choosing an attribute that splits the data in such a way that it that minimizes the impurity of each subset.
Trees are constructed by recursively performing splits on the child subsets until the resulting subset is entirely homogenous or small enough.
Methods for constructing decision trees differ primarily based on how they define impurity and therefore what attributes they choose for each node.
In Classification and Regression Trees (CART), Breiman et al lay out an algorithm to create these trees \cite{BreimanFrOlSt84}.
CART tries to minimize Gini impurity which is a measure of the probability that any random element taken from a node is mislabeled.
Another popular algorithm, C4.5, uses the idea of information gain to make its splits instead \cite{Quinlan93}.
In C4.5, nodes are chosen in such a way that each split minimizes the amount of information necessary to reconstruct the original data.
Both algorithms grow the initial tree greedily and then prune later to avoid overfitting.

While most decision trees are constructed greedily, and thus sub-optimally, there has been some work on constructing optimal decision trees \cite{Moret82}.
There has even been the use of a branch and bound technique in an attempt to construct more optimal decision trees.
Garofalakis et al introduce an algorithm to generate more interpretable decision trees by allowing constraints to be placed on the size of the decision tree \cite{GarofalakisHyRaSh00}.
They use the branch-and-bound technique to constrain the size of the search space and limit the eventual size of the decision tree.
During tree construction, they bound the possible Minimum Description Length (MDL) cost of every different split at a given node.
If every split at that node is more expensive than the actual cost of the current subtree, then that node can be pruned.
In this way, they were able to prune the tree while constructing it instead of just constructing the tree and then pruning at the end.
However, even with the added bounds, this approach did not yield globally optimal decision trees because they constrained the number of nodes in the tree.

Whereas decision trees are always grown from the top downwards, decision lists are built while looking at the entire pool of rules.
Thus, while decision trees are often unable to achieve optimal performance even on simple tasks such as determining the winner of a tic-tac-toe game, decision lists can achieve globally optimal performance.
Decision lists are a generalization of decision trees since any decision tree can be converted to a decision list through the creation of rules to represent the leaves of the decision tree \cite{Rivest87}.
Thus, decision list algorithms are a direct competitor to the popular interpretable methods detailed above: CART and C4.5.
Indeed, decision list algorithms are being used for a number of real world applications including stroke prediction \cite{LethamRuMcMa15}, suggesting medical treatments \cite{ZhangLaTsDa2015}, and text classification \cite{LiYa02}.

Work in the field of decision lists focuses both on the generation of new theoretical bounds and the improvement of predictive accuracy of models.
Recent work on improving accuracy has led to the creation of probabilistic decision lists that generate a posterior distribution over the space of potential decision lists\cite{LethamRuMcMa15,YangRuSe16}.
These methods achieve good accuracy while maintaining a small execution time.
In addition, these methods improve on existing methods such as CART or C4.5 by optimizing over the global space of decision lists as opposed to searching for rules greedily and getting stuck at local optima.
Letham et al are able to do this by pre-mining rules, which reduces the search space from every possible split of the data to a discrete number of rules.
We take the same approach towards optimizing over the global search space, though we don’t use probabilistic techniques.
We also want to work in a regime with a discrete number of rules, thus we use the same rule mining framework from Letham et al to generate the rules for our data sets.
This framework creates features from the raw binary data and then builds rules out of those features.
Yang et al builds on this earlier work by placing additional bounds on the search space and creating a fast low-level framework for computation, specifically a high performance bit vector manipulation library.
We use that bit vector manipulation library to help perform computations involving calculating accuracy of rules.

Our use of a branch and bound technique is inspired by the fact that it is often applied to problems that have a large number of potential solutions without a polynomial time algorithm.
The branch and bound algorithm recursively splits the data into subgroups, yielding a tree-like structure.
Then, by calculating a value corresponding to the end goal of the algorithm (e.g. accuracy), some branches of the tree can be proved to be worse in every case than another branch and therefore can be pruned, reducing the search space.
This technique has been used to solve NP-hard problems such as the Traveling Salesman Problem \cite{LittleMuSwKa63}, the Knapsack Problem \cite{Kolesar67}, or the Mixed Integer Programming problems \cite{Clausen99}.
Branch and Bound is also used as an optimization technique for some machine learning algorithms, though it has not been applied to decision lists before now \cite{ChapelleSiKe06}.