%!TEX root = ../dissertation.tex
%\begin{savequote}[75mm]
%Nulla facilisi. In vel sem. Morbi id urna in diam dignissim feugiat. Proin molestie tortor eu velit. Aliquam erat volutpat. Nullam ultrices, diam tempus vulputate egestas, eros pede varius leo.
%\qauthor{Quoteauthor Lastname}
%\end{savequote}

\chapter{Related Work}

The use of classification models is popular in a number of different fields from image recognition \cite{LeCunBoBeHa98} to churn prediction \cite{LemmensCr06}.
Oftentimes, however, simply receiving a prediction from software is not enough--it is important to have a predictive model that humans can investigate and understand \cite{Bratko97, Quinlan99, Ruping06, MartensVaVeBa11, Freitas14}.
For example, in fields such as medical diagnoses \cite{BellazziZu08} and criminal sentencing \cite{LarsonMaKiAn16}, it is important to be able to investigate the reasons behind a model's predictions.
One reason is that medical experts are unlikely to trust the predictions of these models if they are unable to understand why the model is making certain predictions \cite{Lavrač99}.
Interpretable models also allow users to examine predictions to detect systemic biases in the model.
This is especially important in classification problems, such as criminal recidivism prediction, where there are often race-related biases \cite{LarsonMaKiAn16} or credit scoring where a justification is necessary for the denial of credit \cite{BaesensMuDeVaSe05}.

Tree structured classifiers are a popular technique that combines interpretability with high predictive accuracy.
Also called decision trees, these trees are often used as either classification or regression tools.
Every node in the tree classifier splits the data into two subsets; these subsets are then recursively split by nodes lower in the tree.
Nodes are constructed by choosing an attribute that splits the data to minimize a certain metric.
This metric differs from algorithm to algorithm, but it is usually focused on separating similar items into their own groups.%, but one common metric is attempting to  impurity of each subset.
Trees are constructed by recursively performing splits on the child subsets until the resulting subset is either entirely homogenous according to the metric or small enough according to some threshold.
Methods for constructing decision trees differ primarily based on how they define this metric and what attributes they choose for each node.
Breiman et al. laid out an seminal algorithm, CART, to create such trees \cite{BreimanFrOlSt84}.
CART tries to minimize Gini impurity which is a measure of the probability that any random element taken from a node is mislabeled.
Another popular algorithm, C$4.5$, uses the idea of information gain to make its splits instead \cite{Quinlan93}.
In C$4.5$, nodes are chosen such that each split minimizes the amount of information necessary to reconstruct the original data.
Both algorithms grow the initial tree greedily.
However, this leads to extremely large trees, so they perform a post-processing step of pruning to avoid overfitting and maintain interpretability.

While most decision trees are constructed greedily, and thus sub-optimally, there has been some work on constructing optimal decision trees \cite{Moret82}.
There has even been the use of a branch and bound technique in an attempt to construct more optimal decision trees.
Garofalakis et al. introduce an algorithm to generate more interpretable decision trees by allowing constraints to be placed on the size of the decision tree \cite{GarofalakisHyRaSh00}.
They use the branch-and-bound technique to constrain the size of the search space and limit the eventual size of the decision tree.
During tree construction, they bound the possible Minimum Description Length (MDL) cost of every different split at a given node.
If every split at that node leads to a more expensive tree than the MDL cost of the current subtree, then that node can be pruned.
In this way, they were able to prune the tree while constructing it instead of just constructing the tree and then pruning at the end.
However, even with the added bounds, this approach did not yield globally optimal decision trees, because they constrained the number of nodes in the tree.

Whereas decision trees are always grown from the top downwards, decision lists are built while considering the entire pool of rules.
Thus, while decision trees are often unable to achieve optimal performance even on simple tasks such as determining the winner of a tic-tac-toe game, decision lists can achieve globally optimal performance.
Decision lists are a generalization of decision trees since any decision tree can be converted to a decision list through the creation of rules to represent the leaves of the decision tree \cite{Rivest87}.
Thus, decision list algorithms are a direct competitor to the popular interpretable methods detailed above: CART and C$4.5$.
Indeed, decision list algorithms are being used for a number of real world applications including stroke prediction \cite{LethamRuMcMa15}, suggesting medical treatments \cite{ZhangLaTsDa2015}, and text classification \cite{LiYa02}.

Work in the field of decision lists focuses both on the generation of new theoretical bounds and the improvement of predictive accuracy of models.
Recent work on improving accuracy has led to the creation of probabilistic decision lists that generate a posterior distribution over the space of potential decision lists \cite{LethamRuMcMa15,YangRuSe16}.
These methods achieve good accuracy while maintaining a small execution time.
In addition, these methods improve on existing methods, such as CART or C4.5, by optimizing over the global space of decision lists as opposed to searching for rules greedily and getting stuck at local optima.
Letham et al. are able to do this by pre-mining rules, which reduces the search space from every possible split of the data to a discrete number of rules.
We take the same approach towards optimizing over the global search space, though we don’t use probabilistic techniques.
We also want to work in a regime with a discrete number of rules, thus we use the same rule mining framework from Letham et al. to generate the rules for our data sets \cite{LethamRuMcMa15}.
This framework creates features from the raw binary data and then builds rules out of those features.
Yang et al. builds on this earlier work by placing additional bounds on the search space and creating a fast low-level framework for computation, specifically a high performance bit vector manipulation library.
We use that bit vector manipulation library to help perform computations involving calculating accuracy of rules \cite{YangRuSe16}.

Branch and bound was a technique originally developed to solve linear programming problems \cite{LandDo60}.
The branch and bound algorithm recursively splits the data into subgroups, yielding a tree-like structure.
Then, by calculating a value corresponding to the end goal of the algorithm (e.g., accuracy), some branches of the tree can be proved to be worse in every case than another branch and therefore can be pruned, reducing the search space.
For decades, it has been used to great effect in the realm of mixed integer programming \cite{LinderothSa99}.
It has also been applied to other NP-hard problems such as the Traveling Salesman Problem \cite{LittleMuSwKa63} and the Knapsack Problem \cite{Kolesar67}.
More recently, it has been applied to machine learning problems such as feature subset selection \cite{NarendraFu77} and clustering \cite{NarendraFu75}.
However, in recent times its popularity has declined in favor of convex optimization methods such as neural nets.

TODO: Complete paragraph about discrete optimization/bounds in 90s

In the $1980$s and $90$s, soon after the release of Valiant's theory of learnability \cite{Valiant84} and Rivest's development of decision lists \cite{Rivest87}, there were a host of papers concerned with bounding the efficiency of learning boolean functions (which are equivalent to decision lists).
Blum \cite{Blum90}
\cite{DhagatHe94}